defaults:
  - _self_

env:
  id: "halfcheetah-medium-replay-v2"
  action_size: 6
  state_size: 17
  seed: 0

device: "cuda:0"
algorithm:
  type: "GMPO"
  solver_type: "ODESolver"
  model_type: "DiffusionModel"
  generative_model_type: "GVP"
  model_loss_type: "flow_matching"

path:
  type: "gvp"

project_name: "d4rl-${env.id}-${algorithm.type}-${algorithm.generative_model_type}"

model:
  device: ${device}
  x_size: ${env.action_size}
  solver:
    type: "ODESolver"
    args:
      library: "torchdiffeq"
  path: ${path}
  reverse_path: ${path}
  model:
    type: "velocity_function"
    args:
      t_encoder:
        type: "GaussianFourierProjectionTimeEncoder"
        args:
          embed_dim: 32
          scale: 30.0
      backbone:
        type: "TemporalSpatialResidualNet"
        args:
          hidden_sizes: [512, 256, 128]
          output_dim: ${env.action_size}
          t_dim: 32
          condition_dim: ${env.state_size}
          condition_hidden_dim: 32
          t_condition_hidden_dim: 128

train:
  project: ${project_name}
  device: ${device}
  seed: ${env.seed}
  wandb:
    project: "IQL-${env.id}-${algorithm.type}-${algorithm.generative_model_type}"
  simulator:
    type: "GymEnvSimulator"
    args:
      env_id: ${env.id}
  dataset:
    type: "GPD4RLTensorDictDataset"
    args:
      env_id: ${env.id}
  model:
    GPPolicy:
      device: ${device}
      model_type: ${algorithm.model_type}
      model_loss_type: ${algorithm.model_loss_type}
      model: ${model}
      critic:
        device: ${device}
        q_alpha: 1.0
        DoubleQNetwork:
          backbone:
            type: "ConcatenateMLP"
            args:
              hidden_sizes:  [23, 256, 256]
              output_size: 1
              activation: "relu"
        VNetwork:
          backbone:
            type: "MultiLayerPerceptron"
            args:
              hidden_sizes:  [17, 256, 256]
              output_size: 1
              activation: "relu"
    GuidedPolicy:
      model_type: ${algorithm.model_type}
      model: ${model}
  parameter:
    algorithm_type: ${algorithm.type}
    behaviour_policy:
      batch_size: 4096
      learning_rate: 1e-4
      epochs: 0
    t_span: 32
    critic:
      batch_size: 4096
      epochs: 12000
      learning_rate: 3e-4
      discount_factor: 0.99
      update_momentum: 0.005
      tau: 0.7
      method: "iql"
    guided_policy:
      batch_size: 4096
      epochs: 10000
      learning_rate: 1e-4
      beta: 4.0
      weight_clamp: 400
    evaluation:
      eval: true
      repeat: 5
      interval: 100
    checkpoint_path: "./${project_name}/checkpoint"
    checkpoint_freq: 10

deploy:
  device: ${device}
  env:
    env_id: ${env.id}
    seed: ${env.seed}
  t_span: 32