from abc import abstractmethod
from typing import List

import gym
import numpy as np
import torch
from tensordict import TensorDict

from torchrl.data import LazyTensorStorage, LazyMemmapStorage
from torch_geometric.data import InMemoryDataset, Data
from grl.utils.log import log
from grl.datasets.gp import GPD4RLDataset

from torch import Tensor
from torch_geometric.data import Batch
from torch_geometric.utils import cumsum
from torch_geometric.data.storage import NodeStorage

from typing_extensions import Self
from collections import defaultdict
from typing import (
    Dict,
    List,
    Optional,
    TypeVar,
    Union,
)

T = TypeVar('T')
SliceDictType = Dict[str, Union[Tensor, Dict[str, Tensor]]]
IncDictType = Dict[str, Union[Tensor, Dict[str, Tensor]]]



class D4RLGraphDataset(InMemoryDataset):
    """
    Overview:
        D4RL Dataset for Generative Policy algorithm.
    Interface:
        ``__init__``, ``__getitem__``, ``__len__``.
    """

    def __init__(
        self,
        env_id: str,
        action_augment_num: int = None,
    ):
        """
        Overview:
            Initialization method of GPD4RLDataset class
        Arguments:
            env_id (:obj:`str`): The environment id
        """

        super().__init__()
        import d4rl

        data = d4rl.qlearning_dataset(gym.make(env_id))
        self.states = torch.from_numpy(data["observations"]).float()
        self.actions = torch.from_numpy(data["actions"]).float()
        self.next_states = torch.from_numpy(data["next_observations"]).float()
        reward = torch.from_numpy(data["rewards"]).view(-1, 1).float()
        self.is_finished = torch.from_numpy(data["terminals"]).view(-1, 1).float()

        reward_tune = "iql_antmaze" if "antmaze" in env_id else "iql_locomotion"
        if reward_tune == "normalize":
            reward = (reward - reward.mean()) / reward.std()
        elif reward_tune == "iql_antmaze":
            reward = reward - 1.0
        elif reward_tune == "iql_locomotion":
            min_ret, max_ret = GPD4RLDataset.return_range(data, 1000)
            reward /= max_ret - min_ret
            reward *= 1000
        elif reward_tune == "cql_antmaze":
            reward = (reward - 0.5) * 4.0
        elif reward_tune == "antmaze":
            reward = (reward - 0.25) * 2.0
        self.rewards = reward
        self.len = self.states.shape[0]
        log.info(f"{self.len} data loaded in GPD4RLDataset")
        # self.storage.set(
        #     range(self.len), TensorDict(
        #         {
        #             "s": self.states,
        #             "a": self.actions,
        #             "r": self.rewards,
        #             "s_": self.next_states,
        #             "d": self.is_finished,
        #         },
        #         batch_size=[self.len],
        #     )
        # )
        
        self.graph_data = []
        
        def _build_graph(self, s, a, r, s_, d):
            
            data = Data(x=s,
                        edge_index=edge_index,
                        edge_attr=edge_attr,
                        y=s_ #you can add more arguments as you like
                        )
            return data

    def return_range(dataset, max_episode_steps):
        returns, lengths = [], []
        ep_ret, ep_len = 0.0, 0
        for r, d in zip(dataset["rewards"], dataset["terminals"]):
            ep_ret += float(r)
            ep_len += 1
            if d or ep_len == max_episode_steps:
                returns.append(ep_ret)
                lengths.append(ep_len)
                ep_ret, ep_len = 0.0, 0
        # returns.append(ep_ret)    # incomplete trajectory
        lengths.append(ep_len)  # but still keep track of number of steps
        assert sum(lengths) == len(dataset["rewards"])
        return min(returns), max(returns)


    def __getitem__(self, index):
        """
        Overview:
            Get data by index
        Arguments:
            index (:obj:`int`): Index of data
        Returns:
            data (:obj:`dict`): Data dict
        
        .. note::
            The data dict contains the following keys:
            
            s (:obj:`torch.Tensor`): State
            a (:obj:`torch.Tensor`): Action
            r (:obj:`torch.Tensor`): Reward
            s_ (:obj:`torch.Tensor`): Next state
            d (:obj:`torch.Tensor`): Is finished
            fake_a (:obj:`torch.Tensor`): Fake action for contrastive energy prediction and qgpo training \
                (fake action is sampled from the action support generated by the behaviour policy)
            fake_a_ (:obj:`torch.Tensor`): Fake next action for contrastive energy prediction and qgpo training \
                (fake action is sampled from the action support generated by the behaviour policy)
        """

        data = self.storage.get(index=index)
        return data

    def __len__(self):
        return self.len
    
    @property
    def raw_file_name(self):
        pass
    
    @property
    def processed_file_names(self):
        pass
    
    def download(self):
        pass
    
    def process(self):
        pass



def custom_inc(key, stores):
    if 'edge_index' in key:
        unit = torch.tensor(stores[0].size()).view(2, 1)
        repeats = unit.repeat(stores.shape[0], 1, 1)
    else:
        repeats = torch.zeros(stores.shape[0], dtype=int)
    return cumsum(repeats[:-1])

def repeat_interleave(
    repeats: List[int],
    device: Optional[torch.device] = None,
) -> Tensor:
    outs = [torch.full((n, ), i, device=device) for i, n in enumerate(repeats)]
    return torch.cat(outs, dim=0)


def collate(
    cls,
    batch_data,
    add_batch: bool = True,
):
    
    if cls != batch_data.__class__:  # Dynamic inheritance.
        out = cls(_base_cls=batch_data.__class__)  # type: ignore
    else:
        out = cls()

    # Create empty stores:
    out.stores_as(batch_data)  # type: ignore

    key_to_stores = defaultdict(list)
    for store in batch_data.stores:
        key_to_stores[store._key] = store

    device: Optional[torch.device] = None
    slice_dict: SliceDictType = {}
    inc_dict: IncDictType = {}
    for out_store in out.stores:  # type: ignore
        key = out_store._key
        stores = key_to_stores[key]  # Node/EdgeStorage
        for attr in stores.keys():

            value = stores[attr]  # tensor [Batch, node, feat]

            # The `num_nodes` attribute needs special treatment, as we need to
            # sum their values up instead of merging them to a list:
            if attr == 'num_nodes':
                assert 0, "Incomplete implementation"
                # out_store._num_nodes = value
                # out_store.num_nodes = sum(value)
                # continue

            # Skip batching of `ptr` vectors for now:
            if attr == 'ptr':
                continue

            # Collate attributes into a unified representation:
            slices = torch.arange(0, (value.shape[0]+1)*value.shape[1], value.shape[1])
            incs = custom_inc(attr, value)
            if isinstance(stores, NodeStorage):
                value = value.reshape(-1, value.shape[-1])
            elif attr == 'edge_index':
                assert value.shape[2] == 1, "more than one node is not implemented yet"
                value = torch.transpose(value, 0, 1).reshape(value.shape[1], -1) + 1
                value = cumsum(value, dim=1)[:, :-1].to(torch.int64)

            # If parts of the data are already on GPU, make sure that auxiliary
            # data like `batch` or `ptr` are also created on GPU:
            if isinstance(value, Tensor) and value.is_cuda:
                device = value.device

            out_store[attr] = value

            if key is not None:  # Heterogeneous:
                store_slice_dict = slice_dict.get(key, {})
                assert isinstance(store_slice_dict, dict)
                store_slice_dict[attr] = slices
                slice_dict[key] = store_slice_dict

                store_inc_dict = inc_dict.get(key, {})
                assert isinstance(store_inc_dict, dict)
                store_inc_dict[attr] = incs
                inc_dict[key] = store_inc_dict
            else:  # Homogeneous:
                slice_dict[attr] = slices
                inc_dict[attr] = incs

            # In case of node-level storages, we add a top-level batch vector it:
            if (add_batch and isinstance(stores, NodeStorage) and key == 'x'):
                repeats = torch.ones((value.shape[0]), dtype=int) * value.shape[1]
                out_store.batch = repeat_interleave(repeats, device=device)
                out_store.ptr = cumsum(torch.tensor(repeats, device=device))

    return out, slice_dict, inc_dict


class CustomBatch(Batch):
    @classmethod
    def from_batch_data(
        cls,
        batch_data,
        batch_num
    ) -> Self:
        batch, slice_dict, inc_dict = collate(cls, batch_data)

        batch._num_graphs = batch_num # type: ignore
        batch._slice_dict = slice_dict  # type: ignore
        batch._inc_dict = inc_dict  # type: ignore

        return batch